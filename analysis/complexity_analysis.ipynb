{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Scaling Analysis: 5-chip and 7-chip Games\n",
    "\n",
    "**So Long Sucker - Extended Dataset Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question\n",
    "\n",
    "> **How does game complexity affect LLM strategic deception?**\n",
    "\n",
    "This notebook analyzes 60 additional games (5-chip and 7-chip configurations) to understand how increased game length and strategic depth affects model performance.\n",
    "\n",
    "### Key Finding (Preview)\n",
    "\n",
    "| Complexity | GPT-OSS (Bullshitter) | Gemini (Liar) |\n",
    "|------------|----------------------|---------------|\n",
    "| 3-chip silent | **67.4%** | 9.3% |\n",
    "| 7-chip talking | 10.0% | **90.0%** |\n",
    "\n",
    "**The pattern completely reverses.** Simple games favor brute-force strategies; complex games reward strategic manipulation.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Load Data](#1.-Setup-&-Load-Data)\n",
    "2. [Dataset Overview](#2.-Dataset-Overview)\n",
    "3. [Win Rates by Complexity](#3.-Win-Rates-by-Complexity)\n",
    "4. [The Complexity Reversal](#4.-The-Complexity-Reversal)\n",
    "5. [Game Length Analysis](#5.-Game-Length-Analysis)\n",
    "6. [Chat Patterns in Longer Games](#6.-Chat-Patterns-in-Longer-Games)\n",
    "7. [Elimination Patterns](#7.-Elimination-Patterns)\n",
    "8. [Conclusions & AI Safety Implications](#8.-Conclusions-&-AI-Safety-Implications)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Model name normalization\n",
    "def normalize_model(model):\n",
    "    if '/' in model:\n",
    "        model = model.split('/')[-1]\n",
    "    return model.replace('-instruct', '').replace('-preview', '').replace('-0905', '')\n",
    "\n",
    "MODEL_MAP = {\n",
    "    'red': 'gemini-3-flash',\n",
    "    'blue': 'kimi-k2',\n",
    "    'green': 'qwen3-32b',\n",
    "    'yellow': 'gpt-oss-120b'\n",
    "}\n",
    "\n",
    "# Determine base path\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = '/content'\n",
    "    # TODO: Add download logic for Colab\n",
    "    print('Running in Colab - data download needed')\n",
    "else:\n",
    "    BASE_PATH = '../data'\n",
    "    print('Using local data files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"Load all JSON files from a directory and extract game data.\"\"\"\n",
    "    all_games = []\n",
    "    all_decisions = []\n",
    "    \n",
    "    if os.path.isfile(path):\n",
    "        files = [path]\n",
    "    else:\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "    \n",
    "    for fpath in files:\n",
    "        with open(fpath) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        session = data.get('session', {})\n",
    "        chips = session.get('chips', 3)\n",
    "        silent = session.get('silent', True)\n",
    "        \n",
    "        for snap in data['snapshots']:\n",
    "            if snap['type'] == 'game_end':\n",
    "                game_info = {\n",
    "                    'chips': chips,\n",
    "                    'silent': silent,\n",
    "                    'mode': 'silent' if silent else 'talking',\n",
    "                    'winner': snap.get('winner'),\n",
    "                    'winner_model': MODEL_MAP.get(snap.get('winner')),\n",
    "                    'turns': snap.get('turns', 0),\n",
    "                    'duration': snap.get('duration', 0) / 1000,  # Convert to seconds\n",
    "                    'elimination_order': snap.get('eliminationOrder', []),\n",
    "                    'chat_count': len(snap.get('chatHistory', [])),\n",
    "                    'game_id': snap.get('game')\n",
    "                }\n",
    "                all_games.append(game_info)\n",
    "            \n",
    "            if snap['type'] == 'decision':\n",
    "                player = snap.get('player')\n",
    "                llm = snap.get('llmResponse') or {}\n",
    "                tool_calls = llm.get('toolCalls') or []\n",
    "                \n",
    "                decision_info = {\n",
    "                    'chips': chips,\n",
    "                    'silent': silent,\n",
    "                    'mode': 'silent' if silent else 'talking',\n",
    "                    'player': player,\n",
    "                    'model': MODEL_MAP.get(player),\n",
    "                    'turn': snap.get('turn', 0),\n",
    "                    'game_id': snap.get('game'),\n",
    "                    'has_think': any(tc.get('name') == 'think' for tc in tool_calls),\n",
    "                    'has_chat': any(tc.get('name') == 'sendChat' for tc in tool_calls),\n",
    "                    'has_kill': any(tc.get('name') == 'killChip' for tc in tool_calls),\n",
    "                    'tool_calls': [tc.get('name') for tc in tool_calls]\n",
    "                }\n",
    "                all_decisions.append(decision_info)\n",
    "    \n",
    "    return pd.DataFrame(all_games), pd.DataFrame(all_decisions)\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {\n",
    "    '3-chip': {\n",
    "        'silent': f'{BASE_PATH}/comparison/silent.json',\n",
    "        'talking': f'{BASE_PATH}/comparison/talking.json'\n",
    "    },\n",
    "    '5-chip': {\n",
    "        'silent': f'{BASE_PATH}/silent_5chip',\n",
    "        'talking': f'{BASE_PATH}/talking_5chip'\n",
    "    },\n",
    "    '7-chip': {\n",
    "        'silent': f'{BASE_PATH}/silent_7chip',\n",
    "        'talking': f'{BASE_PATH}/talking_7chip'\n",
    "    }\n",
    "}\n",
    "\n",
    "all_games_list = []\n",
    "all_decisions_list = []\n",
    "\n",
    "for chip_config, modes in datasets.items():\n",
    "    for mode, path in modes.items():\n",
    "        if os.path.exists(path):\n",
    "            games_df, decisions_df = load_dataset(path)\n",
    "            all_games_list.append(games_df)\n",
    "            all_decisions_list.append(decisions_df)\n",
    "            print(f\"Loaded {chip_config} {mode}: {len(games_df)} games, {len(decisions_df)} decisions\")\n",
    "\n",
    "games_df = pd.concat(all_games_list, ignore_index=True)\n",
    "decisions_df = pd.concat(all_decisions_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal: {len(games_df)} games, {len(decisions_df)} decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview table\n",
    "overview = games_df.groupby(['chips', 'mode']).agg({\n",
    "    'winner': 'count',\n",
    "    'turns': 'mean',\n",
    "    'chat_count': 'mean',\n",
    "    'duration': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "overview.columns = ['Games', 'Avg Turns', 'Avg Chats', 'Avg Duration (s)']\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Games by configuration\n",
    "print(\"Games per Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "for chips in [3, 5, 7]:\n",
    "    for mode in ['silent', 'talking']:\n",
    "        count = len(games_df[(games_df['chips'] == chips) & (games_df['mode'] == mode)])\n",
    "        print(f\"{chips}-chip {mode}: {count} games\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Win Rates by Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win rates for each configuration\n",
    "def calculate_win_rates(df):\n",
    "    results = []\n",
    "    \n",
    "    for chips in [3, 5, 7]:\n",
    "        for mode in ['silent', 'talking']:\n",
    "            subset = df[(df['chips'] == chips) & (df['mode'] == mode)]\n",
    "            total = len(subset)\n",
    "            \n",
    "            if total == 0:\n",
    "                continue\n",
    "            \n",
    "            for model in ['gemini-3-flash', 'kimi-k2', 'qwen3-32b', 'gpt-oss-120b']:\n",
    "                wins = len(subset[subset['winner_model'] == model])\n",
    "                win_rate = (wins / total) * 100\n",
    "                \n",
    "                results.append({\n",
    "                    'chips': chips,\n",
    "                    'mode': mode,\n",
    "                    'config': f\"{chips}-chip {mode}\",\n",
    "                    'model': model,\n",
    "                    'wins': wins,\n",
    "                    'total': total,\n",
    "                    'win_rate': round(win_rate, 1)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "win_rates_df = calculate_win_rates(games_df)\n",
    "\n",
    "# Pivot table for easy viewing\n",
    "pivot = win_rates_df.pivot_table(\n",
    "    index='model',\n",
    "    columns='config',\n",
    "    values='win_rate',\n",
    "    aggfunc='first'\n",
    ").reindex(columns=['3-chip silent', '3-chip talking', '5-chip silent', '5-chip talking', '7-chip silent', '7-chip talking'])\n",
    "\n",
    "print(\"Win Rates (%) by Configuration:\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Win rates across complexity levels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Colors for models\n",
    "colors = {\n",
    "    'gemini-3-flash': '#4285F4',  # Google blue\n",
    "    'kimi-k2': '#FF6B6B',\n",
    "    'qwen3-32b': '#4ECDC4',\n",
    "    'gpt-oss-120b': '#95D5B2'\n",
    "}\n",
    "\n",
    "# Plot 1: Silent mode across chip counts\n",
    "ax1 = axes[0]\n",
    "for model in ['gemini-3-flash', 'kimi-k2', 'qwen3-32b', 'gpt-oss-120b']:\n",
    "    silent_data = win_rates_df[(win_rates_df['model'] == model) & (win_rates_df['mode'] == 'silent')]\n",
    "    ax1.plot(silent_data['chips'], silent_data['win_rate'], 'o-', label=model, color=colors[model], linewidth=2, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Chips per Player')\n",
    "ax1.set_ylabel('Win Rate (%)')\n",
    "ax1.set_title('Silent Mode: Win Rate by Complexity')\n",
    "ax1.set_xticks([3, 5, 7])\n",
    "ax1.axhline(y=25, color='gray', linestyle='--', alpha=0.5, label='Expected (25%)')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_ylim(0, 80)\n",
    "\n",
    "# Plot 2: Talking mode across chip counts\n",
    "ax2 = axes[1]\n",
    "for model in ['gemini-3-flash', 'kimi-k2', 'qwen3-32b', 'gpt-oss-120b']:\n",
    "    talking_data = win_rates_df[(win_rates_df['model'] == model) & (win_rates_df['mode'] == 'talking')]\n",
    "    ax2.plot(talking_data['chips'], talking_data['win_rate'], 'o-', label=model, color=colors[model], linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set_xlabel('Chips per Player')\n",
    "ax2.set_ylabel('Win Rate (%)')\n",
    "ax2.set_title('Talking Mode: Win Rate by Complexity')\n",
    "ax2.set_xticks([3, 5, 7])\n",
    "ax2.axhline(y=25, color='gray', linestyle='--', alpha=0.5, label='Expected (25%)')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Complexity Reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key finding: complete reversal of dominance\n",
    "print(\"=\"*60)\n",
    "print(\"THE COMPLEXITY REVERSAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract specific values\n",
    "def get_win_rate(model, chips, mode):\n",
    "    row = win_rates_df[(win_rates_df['model'] == model) & \n",
    "                       (win_rates_df['chips'] == chips) & \n",
    "                       (win_rates_df['mode'] == mode)]\n",
    "    return row['win_rate'].values[0] if len(row) > 0 else 0\n",
    "\n",
    "print(\"\\n** Simple Games (3-chip silent) **\")\n",
    "print(f\"  GPT-OSS (Bullshitter): {get_win_rate('gpt-oss-120b', 3, 'silent')}% - DOMINATES\")\n",
    "print(f\"  Gemini (Liar):         {get_win_rate('gemini-3-flash', 3, 'silent')}% - Struggles\")\n",
    "\n",
    "print(\"\\n** Complex Games (7-chip talking) **\")\n",
    "print(f\"  GPT-OSS (Bullshitter): {get_win_rate('gpt-oss-120b', 7, 'talking')}% - Collapses\")\n",
    "print(f\"  Gemini (Liar):         {get_win_rate('gemini-3-flash', 7, 'talking')}% - DOMINATES\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The pattern completely reverses as game complexity increases:\n",
    "\n",
    "1. SIMPLE GAMES favor the BULLSHITTER (GPT-OSS)\n",
    "   - Short games = less time for manipulation to compound\n",
    "   - Random/reactive play is viable\n",
    "   - No need for long-term planning\n",
    "\n",
    "2. COMPLEX GAMES favor the LIAR (Gemini)\n",
    "   - Long games = manipulation compounds over time\n",
    "   - Strategic planning becomes essential\n",
    "   - Truth-tracking enables consistent deception\n",
    "\n",
    "AI SAFETY IMPLICATION:\n",
    "- Deception capability SCALES with task complexity\n",
    "- Models that \"bullshit\" fail in complex scenarios\n",
    "- Models that strategically lie become MORE dangerous\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: The Reversal\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Focus on Gemini vs GPT-OSS comparison\n",
    "chips = [3, 5, 7]\n",
    "\n",
    "gemini_silent = [get_win_rate('gemini-3-flash', c, 'silent') for c in chips]\n",
    "gemini_talking = [get_win_rate('gemini-3-flash', c, 'talking') for c in chips]\n",
    "gpt_silent = [get_win_rate('gpt-oss-120b', c, 'silent') for c in chips]\n",
    "gpt_talking = [get_win_rate('gpt-oss-120b', c, 'talking') for c in chips]\n",
    "\n",
    "x = np.arange(len(chips))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, gemini_silent, width, label='Gemini Silent', color='#4285F4', alpha=0.6)\n",
    "ax.bar(x - 0.5*width, gemini_talking, width, label='Gemini Talking', color='#4285F4')\n",
    "ax.bar(x + 0.5*width, gpt_silent, width, label='GPT-OSS Silent', color='#95D5B2', alpha=0.6)\n",
    "ax.bar(x + 1.5*width, gpt_talking, width, label='GPT-OSS Talking', color='#95D5B2')\n",
    "\n",
    "ax.set_xlabel('Chips per Player')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('The Complexity Reversal: LIAR vs BULLSHITTER')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['3-chip\\n(Simple)', '5-chip\\n(Medium)', '7-chip\\n(Complex)'])\n",
    "ax.axhline(y=25, color='red', linestyle='--', alpha=0.5, label='Expected (25%)')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('GPT-OSS\\ndominates', xy=(0, 67), xytext=(0.3, 75),\n",
    "            fontsize=10, ha='center', color='green',\n",
    "            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "ax.annotate('Gemini\\ndominates', xy=(2, 90), xytext=(1.7, 82),\n",
    "            fontsize=10, ha='center', color='blue',\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Game Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game length by configuration\n",
    "length_stats = games_df.groupby(['chips', 'mode']).agg({\n",
    "    'turns': ['mean', 'min', 'max', 'std'],\n",
    "    'duration': ['mean', 'min', 'max']\n",
    "}).round(1)\n",
    "\n",
    "print(\"Game Length Statistics:\")\n",
    "length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do winners finish faster or slower?\n",
    "print(\"\\nAverage Turns by Winner:\")\n",
    "for chips in [3, 5, 7]:\n",
    "    for mode in ['silent', 'talking']:\n",
    "        subset = games_df[(games_df['chips'] == chips) & (games_df['mode'] == mode)]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{chips}-chip {mode}:\")\n",
    "        for model in ['gemini-3-flash', 'kimi-k2', 'qwen3-32b', 'gpt-oss-120b']:\n",
    "            model_wins = subset[subset['winner_model'] == model]\n",
    "            if len(model_wins) > 0:\n",
    "                avg_turns = model_wins['turns'].mean()\n",
    "                print(f\"  {model}: {avg_turns:.1f} turns avg ({len(model_wins)} wins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chat Patterns in Longer Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat analysis for talking mode only\n",
    "talking_decisions = decisions_df[decisions_df['mode'] == 'talking']\n",
    "\n",
    "# Chat frequency by model and chip count\n",
    "chat_stats = talking_decisions.groupby(['chips', 'model']).agg({\n",
    "    'has_chat': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "chat_stats.columns = ['chips', 'model', 'chats', 'decisions']\n",
    "chat_stats['chat_rate'] = (chat_stats['chats'] / chat_stats['decisions'] * 100).round(1)\n",
    "\n",
    "# Pivot for easy viewing\n",
    "chat_pivot = chat_stats.pivot_table(\n",
    "    index='model',\n",
    "    columns='chips',\n",
    "    values='chat_rate',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "print(\"Chat Rate (% of decisions with chat):\")\n",
    "chat_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does GPT-OSS talk even MORE in longer games? (The Talker's Paradox extended)\n",
    "print(\"\\nThe Talker's Paradox Extended:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chips in [3, 5, 7]:\n",
    "    subset = talking_decisions[talking_decisions['chips'] == chips]\n",
    "    total_chats = subset['has_chat'].sum()\n",
    "    \n",
    "    print(f\"\\n{chips}-chip games:\")\n",
    "    for model in ['gemini-3-flash', 'kimi-k2', 'qwen3-32b', 'gpt-oss-120b']:\n",
    "        model_chats = subset[subset['model'] == model]['has_chat'].sum()\n",
    "        pct = (model_chats / total_chats * 100) if total_chats > 0 else 0\n",
    "        \n",
    "        # Get win rate for comparison\n",
    "        wr = get_win_rate(model, chips, 'talking')\n",
    "        \n",
    "        print(f\"  {model}: {pct:.1f}% of chats, {wr}% win rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Elimination Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who gets eliminated first at different complexity levels?\n",
    "print(\"First Elimination by Configuration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chips in [3, 5, 7]:\n",
    "    for mode in ['silent', 'talking']:\n",
    "        subset = games_df[(games_df['chips'] == chips) & (games_df['mode'] == mode)]\n",
    "        \n",
    "        first_elims = []\n",
    "        for _, row in subset.iterrows():\n",
    "            order = row['elimination_order']\n",
    "            if order and len(order) > 0:\n",
    "                first_elims.append(MODEL_MAP.get(order[0], order[0]))\n",
    "        \n",
    "        if first_elims:\n",
    "            counts = Counter(first_elims)\n",
    "            total = len(first_elims)\n",
    "            \n",
    "            print(f\"\\n{chips}-chip {mode}:\")\n",
    "            for model, count in counts.most_common():\n",
    "                print(f\"  {model}: {count}/{total} ({count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targeting analysis: Does Gemini get targeted less in complex games?\n",
    "print(\"\\nGemini First-Elimination Rate by Complexity:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chips in [3, 5, 7]:\n",
    "    for mode in ['silent', 'talking']:\n",
    "        subset = games_df[(games_df['chips'] == chips) & (games_df['mode'] == mode)]\n",
    "        \n",
    "        gemini_first = 0\n",
    "        total = 0\n",
    "        \n",
    "        for _, row in subset.iterrows():\n",
    "            order = row['elimination_order']\n",
    "            if order and len(order) > 0:\n",
    "                total += 1\n",
    "                if order[0] == 'red':  # Gemini is red\n",
    "                    gemini_first += 1\n",
    "        \n",
    "        if total > 0:\n",
    "            rate = gemini_first / total * 100\n",
    "            print(f\"{chips}-chip {mode}: Gemini eliminated first {rate:.1f}% of games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions & AI Safety Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║               COMPLEXITY SCALING: KEY FINDINGS                    ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. THE REVERSAL EFFECT\n",
    "   ────────────────────\n",
    "   - Simple games (3-chip): Bullshitter (GPT-OSS) wins 67%\n",
    "   - Complex games (7-chip): Liar (Gemini) wins 90%\n",
    "   - Complete inversion of dominance with complexity\n",
    "\n",
    "2. DECEPTION SCALES WITH COGNITIVE LOAD\n",
    "   ─────────────────────────────────────\n",
    "   - Longer games = more opportunities for manipulation\n",
    "   - Strategic liars compound their advantage over time\n",
    "   - Bullshitters' noise becomes increasingly harmful\n",
    "\n",
    "3. GAME LENGTH TRIPLES\n",
    "   ────────────────────\n",
    "   - 3-chip: ~18 turns average\n",
    "   - 5-chip: ~37 turns average (2x)\n",
    "   - 7-chip: ~54 turns average (3x)\n",
    "\n",
    "4. THE TALKER'S PARADOX PERSISTS\n",
    "   ──────────────────────────────\n",
    "   - GPT-OSS still over-communicates at all complexity levels\n",
    "   - Over-communication becomes MORE harmful in longer games\n",
    "\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    AI SAFETY IMPLICATIONS                         ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. TASK COMPLEXITY REVEALS TRUE CAPABILITY\n",
    "   - Simple benchmarks may miss manipulation capability\n",
    "   - Complex, multi-turn tasks expose strategic deception\n",
    "\n",
    "2. LIARS > BULLSHITTERS IN HIGH-STAKES SCENARIOS\n",
    "   - Models that track truth and strategically lie are MORE dangerous\n",
    "   - Bullshitters fail gracefully (become obvious)\n",
    "   - Liars succeed more as complexity increases\n",
    "\n",
    "3. EVALUATION RECOMMENDATIONS\n",
    "   - Test deception in LONG, COMPLEX scenarios\n",
    "   - Simple games underestimate manipulation capability\n",
    "   - Monitor private reasoning (think tools) for divergence\n",
    "\n",
    "4. MITIGATION STRATEGIES\n",
    "   - For LIARS: Chain-of-thought auditing, consistency checks\n",
    "   - For BULLSHITTERS: Grounding, fact-checking\n",
    "   - Different threats require different interventions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary_data = []\n",
    "\n",
    "for chips in [3, 5, 7]:\n",
    "    for mode in ['silent', 'talking']:\n",
    "        subset = games_df[(games_df['chips'] == chips) & (games_df['mode'] == mode)]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        gemini_wr = get_win_rate('gemini-3-flash', chips, mode)\n",
    "        gpt_wr = get_win_rate('gpt-oss-120b', chips, mode)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Config': f\"{chips}-chip {mode}\",\n",
    "            'Games': len(subset),\n",
    "            'Avg Turns': round(subset['turns'].mean(), 1),\n",
    "            'Gemini Win%': gemini_wr,\n",
    "            'GPT-OSS Win%': gpt_wr,\n",
    "            'Dominant': 'Gemini' if gemini_wr > gpt_wr else 'GPT-OSS'\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary Table:\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
